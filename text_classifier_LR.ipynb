{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "7UyqcsIByMnW",
    "outputId": "7366b68b-de21-4806-9dbb-3168b687f2df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyDrive\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
      "\u001b[K    100% |████████████████████████████████| 993kB 10.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.4)\n",
      "Building wheels for collected packages: PyDrive\n",
      "  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
      "Successfully built PyDrive\n",
      "Installing collected packages: PyDrive\n",
      "Successfully installed PyDrive-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyDrive # 安装一个包\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HDaWXWwyZwj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "# 以下两句代码在 Google Colab 才需要添加，其他平台忽略\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42vLTDz4yPwa"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxGtml9L_b59"
   },
   "outputs": [],
   "source": [
    "# # 2. Auto-iterate using the query syntax\n",
    "# #    https://developers.google.com/drive/v2/web/search-parameters\n",
    "# file_list = drive.ListFile(\n",
    "#     {'q': \"'https://drive.google.com/open?id=1AFxy5ICODGYsdtDYfBzJK6AiVlxyjPHx' in parents\"}).GetList()\n",
    "\n",
    "# for f in file_list:\n",
    "#   # 3. Create & download by id.\n",
    "#   print('title: %s, id: %s' % (f['title'], f['id']))\n",
    "#   fname = os.path.join(local_download_path, f['title'])\n",
    "#   print('downloading to {}'.format(fname))\n",
    "#   f_ = drive.CreateFile({'id': f['id']})\n",
    "#   f_.GetContentFile(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R32lR-Ly_qdN"
   },
   "outputs": [],
   "source": [
    "download = drive.CreateFile({'id':'1AFxy5ICODGYsdtDYfBzJK6AiVlxyjPHx'})\n",
    "download.GetContentFile('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rfF5feuv_9oK",
    "outputId": "7f7f8831-731b-4c1c-92f6-288930f39ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json  sample_data  train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "igH2zD_zHY0y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_i4RHqhvACXq"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "U-gFHdwXIZ7x",
    "outputId": "55336b9d-42e2-4360-b054-5e63b4dced87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>《青蛇》造型师默认新《红楼梦》额妆抄袭（图） 凡是看过电影《青蛇》的人，都不会忘记青白二蛇的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>６．１６日剧榜　＜最后的朋友＞　亮最后杀招成功登顶 《最后的朋友》本周的电视剧排行榜单依然只...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>超乎想象的好看《纳尼亚传奇２：凯斯宾王子》 现时资讯如此发达，搜狐电影评审团几乎人人在没有看...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>吴宇森：赤壁大战不会出现在上集 “希望《赤壁》能给你们不一样的感觉。”对于自己刚刚拍完的影片...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>组图：《多情女人痴情男》陈浩民现场耍宝 陈浩民：外面的朋友大家好，现在是搜狐现场直播，欢迎《...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1\n",
       "0  娱乐  《青蛇》造型师默认新《红楼梦》额妆抄袭（图） 凡是看过电影《青蛇》的人，都不会忘记青白二蛇的...\n",
       "1  娱乐  ６．１６日剧榜　＜最后的朋友＞　亮最后杀招成功登顶 《最后的朋友》本周的电视剧排行榜单依然只...\n",
       "2  娱乐  超乎想象的好看《纳尼亚传奇２：凯斯宾王子》 现时资讯如此发达，搜狐电影评审团几乎人人在没有看...\n",
       "3  娱乐  吴宇森：赤壁大战不会出现在上集 “希望《赤壁》能给你们不一样的感觉。”对于自己刚刚拍完的影片...\n",
       "4  娱乐  组图：《多情女人痴情男》陈浩民现场耍宝 陈浩民：外面的朋友大家好，现在是搜狐现场直播，欢迎《..."
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lN47q87zIw4n"
   },
   "outputs": [],
   "source": [
    "download = drive.CreateFile({'id':'1BmP9IbcJK-XdnKnsP7Ta6cNwZsYpewWt'})\n",
    "download.GetContentFile('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zi6wlG0KJJKG"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbTipt03JSLH"
   },
   "outputs": [],
   "source": [
    "train_df.columns = ['分类', '文章']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jhFklB5kJX-V"
   },
   "outputs": [],
   "source": [
    "download = drive.CreateFile({'id':'1rOI0ufmOz9SWBqt6quFY7xikffqyxh-N'})\n",
    "download.GetContentFile('stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0IiKpm-Jn4M"
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt', encoding='utf8') as file:\n",
    "    stopWord_list = [k.strip() for k in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_f5Lw8bjJrTr"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "42L70fnRJugk",
    "outputId": "eb67de7a-bbee-441b-d6be-49cc12b7d344"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.975 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前1000篇文章分词共花费47.18秒\n",
      "前2000篇文章分词共花费91.91秒\n",
      "前3000篇文章分词共花费185.69秒\n",
      "前4000篇文章分词共花费275.88秒\n",
      "前5000篇文章分词共花费361.28秒\n",
      "前6000篇文章分词共花费451.05秒\n",
      "前7000篇文章分词共花费475.68秒\n",
      "前8000篇文章分词共花费500.67秒\n",
      "前9000篇文章分词共花费538.80秒\n",
      "前10000篇文章分词共花费575.11秒\n",
      "前11000篇文章分词共花费617.60秒\n",
      "前12000篇文章分词共花费663.64秒\n",
      "前13000篇文章分词共花费691.41秒\n",
      "前14000篇文章分词共花费717.52秒\n",
      "前15000篇文章分词共花费753.14秒\n",
      "前16000篇文章分词共花费789.96秒\n",
      "前17000篇文章分词共花费825.81秒\n",
      "前18000篇文章分词共花费860.11秒\n",
      "前19000篇文章分词共花费941.06秒\n",
      "前20000篇文章分词共花费1022.69秒\n",
      "前21000篇文章分词共花费1042.51秒\n",
      "前22000篇文章分词共花费1062.79秒\n",
      "前23000篇文章分词共花费1094.27秒\n",
      "前24000篇文章分词共花费1125.36秒\n"
     ]
    }
   ],
   "source": [
    "stopword_list = [k.strip() for k in open('stopwords.txt', encoding='utf8').readlines() if k.strip() != '']\n",
    "cutWords_list = []\n",
    "i = 0\n",
    "startTime = time.time()\n",
    "for article in train_df['文章']:\n",
    "    cutWords = [k for k in jieba.cut(article) if k not in stopword_list]\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print('前%d篇文章分词共花费%.2f秒' %(i, time.time()-startTime))\n",
    "    cutWords_list.append(cutWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmHJWGGbRehF"
   },
   "outputs": [],
   "source": [
    "with open('cutWords_list.txt', 'w') as file: \n",
    "    for cutWords in cutWords_list:\n",
    "        file.write(' '.join(cutWords) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4FrAtkX3RoWE",
    "outputId": "040753ef-c498-482c-e8a4-0294815017a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc.json  cutWords_list.txt  sample_data  stopwords.txt  test.txt  train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYKIjvxBRur1"
   },
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uk7H9qgzRxJM"
   },
   "outputs": [],
   "source": [
    "with open('cutWords_list.txt') as file:\n",
    "    cutWords_list = [k.split() for k in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnSWUiGZSAc0"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJXLPS86SHuM"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rgPls2VSMc-"
   },
   "outputs": [],
   "source": [
    "stopword_list = [k.strip() for k in open('stopwords.txt', encoding='utf8').readlines() if k.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9FAPgiOSPvB"
   },
   "outputs": [],
   "source": [
    "#tfv = TfidfVectorizer(min_df=2,  max_features=None,  ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1, stop_words = stopword_list)\n",
    "tfv = TfidfVectorizer(cutWords_list, stop_words=stopword_list, min_df=40, max_df=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpAG00IKSU28"
   },
   "outputs": [],
   "source": [
    "temp_inputlist = [\" \".join(x) for x in cutWords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "934v4lQiSY01",
    "outputId": "e4f7cda0-8cf9-43d4-a5a2-ad12428e3ed8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['nbsp'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "        input=[['青蛇', '造型师', '默认', '新', '红楼梦', '额妆', '抄袭', '图', '看过', '电影', '青蛇', '不会', '忘记', '青白', '二蛇', '经典', '造型', '飘逸', '身材', '妩媚', '妆容', '独特', '取材于', '京剧', '女角', '头型', '精华', '处', '片子', '称之为', '额妆', '１', '３', '年', '今天', '新版', '红楼梦', '定妆', '照里', '看到', '这种', '熟悉', '妆容', '片子', '昨日', '知情人', '记者', '报料', '叶锦添...销', '车旅费', '上调', '手机', '报销', '费用', '灾区', '员工', '安抚', '慰问', '措施', '紧锣密鼓', '筹划', '之中', '责任编辑', '赵文琳']],\n",
       "        lowercase=True, max_df=0.3, max_features=None, min_df=40,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['aboard', 'about', 'about', 'about', 'above', 'above', 'according', 'according to', 'across', 'across', 'afore', 'after', 'after', 'after', 'afterwards', 'again', 'against', 'against', 'against', 'agin', 'all', 'all', 'almost', 'almost', 'alone', 'along', 'along', 'alongside', 'already',... '）', '，', '：', '：', '；', '；', '？', '？', '？', '＿', '￣', '&nbsp;', '&nbsp', 'pp', 'px', '摄于', 'info'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv.fit(temp_inputlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUiZ3XqnS2ny"
   },
   "outputs": [],
   "source": [
    "X_all = tfv.transform(temp_inputlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "1cMrbRJaSu-l",
    "outputId": "ae4ff7e8-c536-453e-e4f8-7debbfcfe1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小: 27752\n",
      "(24000, 27752)\n"
     ]
    }
   ],
   "source": [
    "print('词表大小:', len(tfv.vocabulary_))\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7239jINTK6j"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_df = pd.read_csv('train.txt', sep='\\t', header=None)\n",
    "labelEncoder = LabelEncoder()\n",
    "y = labelEncoder.fit_transform(train_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "EEv-cZbwTfxR",
    "outputId": "99f15a0d-2b7c-459c-9e95-71aff26e2e7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8589583333333334"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_all, y, test_size=0.2)\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "logistic_model.fit(train_X, train_y)\n",
    "logistic_model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_F9o6qg5XZeg"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tfv.model', 'wb') as file:\n",
    "    save = {\n",
    "        'labelEncoder' : labelEncoder,\n",
    "        'tfidfVectorizer' : tfv,\n",
    "        'logistic_model' : logistic_model\n",
    "    }\n",
    "    pickle.dump(save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "qYLfL4q5XvmQ",
    "outputId": "9ad5cf65-a471-4b2c-ce78-49db124c68a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".   adc.json  cutWords_list.txt  stopwords.txt\ttfidf.model  train.txt\n",
      "..  .config   sample_data\t test.txt\ttfv.model\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJM4pRaZYD0q"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tfv.model', 'rb') as file:\n",
    "    tfidf_model = pickle.load(file)\n",
    "    tfidfVectorizer = tfidf_model['tfidfVectorizer']\n",
    "    labelEncoder = tfidf_model['labelEncoder']\n",
    "    logistic_model = tfidf_model['logistic_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "6e3sX921Y2Qd",
    "outputId": "f2c66441-bbc9-4a18-d30e-378f33de42be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['nbsp'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.txt', sep='\\t', header=None)\n",
    "X = tfidfVectorizer.transform(train_df[1])\n",
    "y = labelEncoder.transform(train_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "sEHd-r3SY85r",
    "outputId": "385ceb4a-e1d6-4283-c3dc-9c932b151e58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81972222 0.81666667 0.81527778 0.81972222 0.82194444]\n",
      "0.8186666666666665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "cv_split = ShuffleSplit(n_splits=5, test_size=0.3)\n",
    "score_ndarray = cross_val_score(logistic_model, X, y, cv=cv_split)\n",
    "print(score_ndarray)\n",
    "print(score_ndarray.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "colab_type": "code",
    "id": "VmfYplzkZfXB",
    "outputId": "86494e68-54f6-4e3a-ab55-ca094f0f7d3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>体育</th>\n",
       "      <th>健康</th>\n",
       "      <th>女人</th>\n",
       "      <th>娱乐</th>\n",
       "      <th>房地产</th>\n",
       "      <th>教育</th>\n",
       "      <th>文化</th>\n",
       "      <th>新闻</th>\n",
       "      <th>旅游</th>\n",
       "      <th>汽车</th>\n",
       "      <th>科技</th>\n",
       "      <th>财经</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>体育</th>\n",
       "      <td>385</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>健康</th>\n",
       "      <td>4</td>\n",
       "      <td>308</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>女人</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>320</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>娱乐</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>371</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>房地产</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>321</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>教育</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>332</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文化</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>326</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>新闻</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>362</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>旅游</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>汽车</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>374</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>科技</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>财经</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      体育   健康   女人   娱乐  房地产   教育   文化   新闻   旅游   汽车   科技   财经\n",
       "体育   385    1   10    5    1    6   12    1    3    2    4    5\n",
       "健康     4  308    9    3    4    9   13    5   14    1    6    6\n",
       "女人     3    7  320   16    3    3   12    2    3    3    1    0\n",
       "娱乐     4    2    6  371    0    2   14    2    4    1    3    2\n",
       "房地产    3    7    7    5  321    5    4    9    7    2    8   19\n",
       "教育     1    7    4    0    2  332   16    3    9    4    3    4\n",
       "文化     3    7   10   16    2    4  326    6    7    3    6    1\n",
       "新闻     3   10    8    2    5    3    8  362   10    1    4   11\n",
       "旅游     2   14    7    5   15    3   21    8  329    1   10    0\n",
       "汽车     1    3    7    0    5    4    3    2    4  374    5    2\n",
       "科技     2   11    8    4   13    6    4    5    8    1  320    8\n",
       "财经     0    8    4    1   11    4    2   14    5    1   11  323"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "logistic_model = LogisticRegressionCV(multi_class='multinomial', solver='lbfgs')\n",
    "logistic_model.fit(train_X, train_y)\n",
    "predict_y = logistic_model.predict(test_X)\n",
    "pd.DataFrame(confusion_matrix(test_y, predict_y), \n",
    "             columns=labelEncoder.classes_, \n",
    "             index=labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "pH7fiMcFdbh1",
    "outputId": "b95f927a-7462-42ed-d07c-06a3979cb47c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>0.934917</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.919715</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>健康</td>\n",
       "      <td>0.789687</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.785319</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女人</td>\n",
       "      <td>0.824427</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>0.876228</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.884044</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>房地产</td>\n",
       "      <td>0.839468</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.830131</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>教育</td>\n",
       "      <td>0.862845</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.852807</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>文化</td>\n",
       "      <td>0.760967</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.803023</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>新闻</td>\n",
       "      <td>0.827795</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.824887</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>旅游</td>\n",
       "      <td>0.838437</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.815614</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>汽车</td>\n",
       "      <td>0.942736</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>0.915080</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>科技</td>\n",
       "      <td>0.786133</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>财经</td>\n",
       "      <td>0.831663</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.830831</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>总体</td>\n",
       "      <td>0.842942</td>\n",
       "      <td>0.841333</td>\n",
       "      <td>0.841721</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label  Precision    Recall        F1  Support\n",
       "0      体育   0.934917  0.905000  0.919715     1000\n",
       "1      健康   0.789687  0.781000  0.785319     1000\n",
       "2      女人   0.824427  0.864000  0.843750     1000\n",
       "3      娱乐   0.876228  0.892000  0.884044     1000\n",
       "4     房地产   0.839468  0.821000  0.830131     1000\n",
       "5      教育   0.862845  0.843000  0.852807     1000\n",
       "6      文化   0.760967  0.850000  0.803023     1000\n",
       "7      新闻   0.827795  0.822000  0.824887     1000\n",
       "8      旅游   0.838437  0.794000  0.815614     1000\n",
       "9      汽车   0.942736  0.889000  0.915080     1000\n",
       "10     科技   0.786133  0.805000  0.795455     1000\n",
       "11     财经   0.831663  0.830000  0.830831     1000\n",
       "999    总体   0.842942  0.841333  0.841721    12000"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def eval_model(y_true, y_pred, labels):\n",
    "    # 计算每个分类的Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    # 计算总体的平均Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision': p,\n",
    "        u'Recall': r,\n",
    "        u'F1': f1,\n",
    "        u'Support': s\n",
    "    })\n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label': ['总体'],\n",
    "        u'Precision': [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1': [tot_f1],\n",
    "        u'Support': [tot_s]\n",
    "    })\n",
    "    res2.index = [999]\n",
    "    res = pd.concat([res1, res2])\n",
    "    return res[['Label', 'Precision', 'Recall', 'F1', 'Support']]\n",
    "\n",
    "predict_y = logistic_model.predict(test_X)\n",
    "eval_model(test_y, predict_y, labelEncoder.classes_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classifier_ok.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
